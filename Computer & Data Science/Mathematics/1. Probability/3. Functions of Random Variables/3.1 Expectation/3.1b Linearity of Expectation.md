>[!note] Linearity of Expectation
>For two independent random variables $X$ and $Y$ in the same probability space and two constants $c_1$ and $c_2$,
>$$\mathbb{E}[c_1X+c_2Y]=c_1\mathbb{E}[X]+c_2\mathbb{E}[Y].$$
#### Proof.
The linearity of [[3.1a Expectation|expectation]] can be proved by taking $\mathbb{P}[X=a]=\sum^{}_{\upomega \in \Upomega}\mathbb{P}[\upomega]$ (sum to one rule). Then taking the definition of a random variable, we get:
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[X] &= \sum^{}_{a \in \mathscr{A}}a \times \mathbb{P}[X=a] \\ 
&= \sum^{}_{\upomega \in \Upomega} X(\upomega) \times \mathbb{P}[\upomega]
\end{aligned}
\end{equation}
$$
Then, for [[2.3 Independent Random Variables|independent random variables]] $X$ and $Y$,
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[X+Y] &= \sum^{}_{\upomega \in \Upomega}(X+Y)(\upomega) \times \mathbb{P}[\upomega] \\ 
&= \sum^{}_{\upomega \in \Upomega} (X(\upomega)+Y(\upomega)) \times \mathbb{P}[\upomega] \\
&= \sum^{}_{\upomega \in \Upomega}X(\upomega) \times \mathbb{P}[\upomega] + \sum^{}_{\upomega \in \Upomega}Y(\upomega) \times \mathbb{P}[\upomega] \\
&= \mathbb{E}[X]+\mathbb{E}[Y]
\end{aligned}
\end{equation}
$$
Why does the probability $\mathbb{P}[\upomega]$ not change with the amount of variables? Well, because we simplify each event $X=a$ into $\upomega \in \Upomega$. All of the above stems from this simple fact, since by the definition of the random variable, $\set{\upomega \in \Upomega : X(\upomega) = a}$ and $\set{(a,\mathbb{P}[X=a]):a \in \mathscr{A}}$. From there, we replace $a$ with $X(\upomega)$, and therefore $\mathbb{P}[X=a]$ with $\mathbb{P}[\upomega]$.

>[!example] More Roulette
>Suppose our net winnings on roulette are $X_n=Y_1+Y_2+...+Y_n$, where each $Y_i$ are the winnings for the $i$th spin. We know from the last note that $\mathbb{E}[Y_i]=-\frac{1}{19}$ for each $i$. Therefore,
>$$\mathbb{E}[X_n]=-\frac{n}{19}.$$
>For example, if we play 1000 games, then $n=1000$ and $\mathbb{E}[X_n]=-\frac{1000}{19} \approx -53$, so you would expect to lose \$53.

>[!example] Coin Toss (Binomial Expectation)
>We toss a coin with probability of getting heads $\mathbb{P}[H]=p$. We assign an indicator random variable $I_i$ that has a value of $1$ if the $ith$ toss is heads, and $0$ if the $i$th toss is tails. We summarize the number of heads as $X_n=I_1+I_2+...+I_n$. Then, $\mathbb{E}[I_i]=1 \times \frac{1}{2} + 0 \times \frac{1}{2}=\frac{1}{2}$ and,
>$$\mathbb{E}[X_n]=\sum^{n}_{i=1}\frac{1}{2}=\frac{n}{2}.$$
>Because of the random variable being either 1 or 0, the expectation of any $X \enspace \textasciitilde \enspace \text{Bin}(n,p)$ is $\mathbb{E}[X]=np.$

> [!example] Balls and Bins
> We want to throw $m$ balls into $n$ bins. Let $X$ be the number of balls thrown in the first bin. Then $X$ behaves like a biased coin tossing $m$ heads with a probability of $\frac{1}{n}$ because the $i$th ball either lands in the first bin, or does not land in the first bin. Following this, $\mathbb{E}[X]=\frac{m}{n}$. 

