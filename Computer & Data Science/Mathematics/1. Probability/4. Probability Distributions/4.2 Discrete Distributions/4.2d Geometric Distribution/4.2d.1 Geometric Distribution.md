>[!note] Geometric Distribution
>For random variable $X$ and $i \in \mathbb{Z}^+$,
>$$\mathbb{P}[X=i]=p(1-p)^{i-1}.$$
>Denoted as $X \enspace \textasciitilde \enspace \text{Geometric}(p)$.
#### Explanation
The geometric distribution describes an elaboration of a [[4.2a Bernoulli Distribution|Bernoulli distribution]] where the trial is repeated until the event with probability $p$ occurs.
#### Expectation ($\mathbb{E}[X] = \frac{1}{p}$)
We start by setting up our equation for solving the expectation using the [[3.1c Tail Sum Formula|tail sum formula]] by saying that
$$
\begin{equation}
\begin{aligned}
\mathbb{P}[X \geq i] &= \sum^{\infty}_{j=i}\mathbb{P}[X=j] \\
&=\sum^{\infty}_{j=i}p(1-p)^{j-1} \\
&=\frac{p}{1-p}\sum^{\infty}_{j=i}(1-p)^{j} \\
\end{aligned}
\end{equation}
$$
Using the definition of the infinite geometric sum $S_n=\sum^{\infty}_{i=0}a_ir^i=\frac{a_1}{1-r}$ with $a_1=(1-p)^i$ and $r=1-p$ (a is not $1$ since geometric series start at $j=1$, and it is not $1-p$ since $j=i$, we are led to,
$$
\begin{equation}
\begin{aligned}
\mathbb{P}[X \geq i] &= \frac{p}{1-p}\sum^{\infty}_{j=i}(1-p)^{j} \\
&= \frac{p}{1-p}\frac{(1-p)^i}{1-(1-p)} \\
&= (1-p)^{i-1}
\end{aligned}
\end{equation}
$$
This means that for large $n$, the probability that we need $i$ trials or more before the first success is determined by $\mathbb{P}[X \geq i] = (1-p)^{i-1}$. Then the probability that we will get a success in less than $i$ trials is $\mathbb{P}[X < i] = 1 - \mathbb{P}[X \geq i]$. 

Let's put this in perspective using a biased coin with a probability of heads $p=\frac{1}{3}$. The probability that we need 1 toss or more before the first success is 1. That we will need 2 or more is $1-p=\frac{2}{3}$. The probability that we will need 3 or more tosses before a success is $(1-p)^2=\frac{4}{9}$. The probability that we will get a successful trial before 3 tosses (so in the first 2 tosses) is $1-\frac{4}{9}=\frac{5}{9}$. Looking at the outcome space for 3 trials or success, we can see why:
$$\Upomega=\set{TTT, TTH, TH, H}$$
Here, $\mathbb{P}[X=3]=\frac{1}{3}(\frac{2}{3})^2=\frac{4}{27}$. This is confirmed by $\mathbb{P}[X=3]=\mathbb{P}[TTH]$. Therefore, $\mathbb{P}[X < 3]=\mathbb{P}[TH]+\mathbb{P}[H]=\frac{1}{3}\frac{2}{3}+\frac{1}{3}=\frac{5}{9}$, which is equal to $1 - (1-p)^2=\frac{5}{9}$.

We now calculate the expectation of the geometric distribution using the [[3.1c Tail Sum Formula|tail sum formula]] and the previously introduced infinite geometric sum as follows:
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[X] &= \sum^{\infty}_{i=1}\mathbb{P}[X \geq i] \\
&= \sum^{\infty}_{i=1}(1-p)^{i-1} \\
&= \frac{(1-p)^{1-1}}{1-(1-p)} \\
&= \frac{1}{p}
\end{aligned}
\end{equation}
$$
Therefore, $\mathbb{E}[X]=\frac{1}{p}$ for a geometric distribution.

#### Variance ($Var(X)=\frac{1-p}{p^2}$)
We already know that $\mathbb{E}[X]=\frac{1}{p}$. Therefore, using $Var(X)=\mathbb{E}[X^2]-E[X]^2$, we only need to find $E[X^2]$. We proceed as follows:
$$
\begin{equation}
\begin{aligned}
\mathbb{E}[X^2] &= \mathbb{E}[X(X-1)]+\mathbb{E}[X]-\mathbb{E}[X]^2
\end{aligned}
\end{equation}
$$
