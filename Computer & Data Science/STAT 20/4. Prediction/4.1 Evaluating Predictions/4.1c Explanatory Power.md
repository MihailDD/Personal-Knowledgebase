>[!Theorem]
>We can define explanatory power as
>$$R^2=\frac{\sum^{n}_{i=1}(\hat{y}_i-\bar{y})^2}{\sum^{n}_{i=1}(y_i-\bar{y})^2}.$$
>$0 \leq R^2 \leq 1$: a lower $R^2$ means predictions are less accurate.
### Intuition
Before we construct a linear prediction model, we need to make a naive prediction using another type of summary. Usually this is the mean, denoted by $\bar{y}$.

We then must measure the degree to which this naive prediction $\bar{y}$ explains away variability in $y_i$. We must first define two more expressions, the **Sum of Squares Due to Regression**, and the **Total Sum of Squares**.
$$SSR=\sum^{n}_{i=1}(\hat{y}_i-\bar{y})^2$$
The s**um of squares due to regression** explains how much variance was removed due to the regression model. It makes sense: it is the squared sum of residual between each of the points on the linear model and the mean. 
$$TSS = \sum^{n}_{i=1}(y_i-\bar{y})^2$$
The total sum of squares represents the total variability of the data from the mean. It is basically variance without the normalization (division by $\frac{1}{n-1}$).

Here is a chart representing all 3 of the statistics (the third one is the Residual Sum of Squares).
<img src="https://i.ibb.co/hHm011r/Screenshot-2023-12-14-at-4-45-22.png" style="width: 50%; margin-left: auto; margin-right: auto; display: block;" >
Because $TSS$ is the total sum, and $SSR$ is the sum due to regression, we can find $SSR$ as a proportion of $TSS$:
$$R^2= \frac{SSR}{TSS}=\frac{\sum^{n}_{i=1}(\hat{y}_i-\bar{y})^2}{\sum^{n}_{i=1}(y_i-\bar{y})^2}$$
	["Of all the variability in your y-variables, what proportion is explained using the linear model?"]

Since SSR is the opposite of the RSS, and SSR+RSS make up TSS, we can find $R^2$ using RSS:
$$R^2=1-\frac{RSS}{TSS} = \frac{\sum^{n}_{i=1}(y_i - \hat{y}_i)^2}{\sum^{n}_{i=1}(y_i-\bar{y})^2}$$