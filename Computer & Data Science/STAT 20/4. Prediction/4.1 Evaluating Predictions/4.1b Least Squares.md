>[!note] Least Squares Optimization
>The least (minimized) squares equations for $b_0$ and $b_1$ are:
>- $$\text{Slope: } b_1=\frac{s_y}{s_x}r$$
>Where $s_i$ is the standard deviation for variable $i$, and $r$ is the correlation coefficient of the variables.
>- $$\text{Intercept: } b_0=\bar{y}-b_1\bar{x}$$
>Where $(\bar{x}, \bar{y})$ is the mean (mid) point in the scatter plot since the least squares line has to pass through it.

### Intuition
We can express the RSS formula as a function of the slope ($b_1$) and the y-intercept ($b_0$) of the linear model using $\hat{y}_i = b_0+b_1x_i$:
$${RSS}(b_0,b_1)=\sum^{n}_{i=1}[y_i-(b_0+b_1x_i)]^2$$
We must minimize this model for $b_0$ and $b_1$ in order to find the linear model. Two ways to do this is using the newer **Gradient Descent** (which also fits deep-learning models), or the older **Nelder-Mead Algorithm** which works by taking the smallest midpoint on a n-dimensional simplex (a 2D triangle or a 3D pyramid), and transforming it to the most optimal (downhill) point. When optimized, we get the least squares solution.