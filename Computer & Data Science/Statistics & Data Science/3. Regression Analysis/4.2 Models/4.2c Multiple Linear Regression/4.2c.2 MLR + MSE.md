>[!note] Least Squares Estimate
>When fitted using [[4.1b Squared Loss|L2 loss]], the general solution to [[4.2c.1 Multiple Linear Regression|multiple linear regression]] is of the form
>$$\hat{\theta} = (\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T\mathbb{Y}$$
>for an invertible $\mathbb{X}^T \mathbb{X}$ only possible when $\mathbb{X}$ is in full column rank.
# Proof
We first take the original definition of MSE and massage it into vector form using the L2 norm:
$$MSE = \frac{1}{n}\sum^{n}_{i=1}(y_i - \hat{y}_i)^2=\frac{1}{n}(||\mathbb{Y} - \hat{\mathbb{Y}}||_2)^2=\frac{1}{n}(||\mathbb{Y} - \mathbb{X}\theta||_2)^2$$
Since $\hat{\mathbb{Y}} = \mathbb{X}\theta$ is a linear combination of $\mathbb{X}$, $\hat{\mathbb{Y}} \in \mathrm{span}(\mathbb{X})$. To minimize the MSE we need that $\hat{\mathbb{Y}}$ which is closest to $\mathbb{Y}$. It turns out that the closest vector to $\mathbb{Y}$ in $\mathrm{span}(\mathbb{X})$ is the one right below $\mathbb{Y}$. Therefore, $\hat{\mathbb{Y}}$ is the orthogonal projection of $\mathbb{Y}$ onto $Span(\mathbb{X})$. However, the formula for orthogonality (perpendicularity) requires the dot product of two vectors to be equal to zero, or similarly $M^T \vec{v}=\vec{0}$. We can substitute $M$ with the design matrix, and $\vec{v}$ with the residual vector $\vec{e}$ to find $\theta$:
$$\mathbb{X}^T(\mathbb{Y}-\hat{\mathbb{Y}})=\mathbb{X}^T(\mathbb{Y}-\mathbb{X}\theta)=\vec{0} \iff \mathbb{X}^T \mathbb{Y}=\mathbb{X}^T \mathbb{X}\theta$$
Therefore,
$$\theta=(\mathbb{X}^T \mathbb{X})^{-1}\mathbb{X}^T \mathbb{Y}.$$
Note that $\vec{e} \perp Span(\mathbb{X})$, which includes $\mathbb{X}, \hat{\mathbb{Y}}$, their columns and any other vector/matrix $\in Span(\mathbb{X)}$.